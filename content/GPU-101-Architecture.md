+++
title = "GPU 101 - Architecture"
date = 2020-10-15 19:50:58
[taxonomies]
tags = ["gpu"]
+++

**图形处理器单元(GPU)** 主要是指运行高度图形化应用时使用的硬件设备，比如 **3D建模软件** 或 **VDI基础设施**。在消费市场上，GPU多用于加速游戏图形。如今，GPGPU(General Purpose GPU) 是现代高性能计算 (HPC) 场景中加速计算的普遍硬件选择。

会用到 GPU 的领域除了我们熟悉的 HPC，比如会用到图像识别的机器学习方向，也常用在医疗、保险和金融行业相关的垂直领域中使用的 [表格数据 (Tabular Data)](https://www.w3.org/TR/tabular-data-model/) 的计算。

因此，就引出一个问题: 为什么需要使用 GPU 而不是 CPU?

## 延迟与吞吐量

首先，来看看 CPU 和 GPU 的主要区别。

CPU 对速度与延迟方面进行了优化，在保持操作之间快速切换的能力的前提下，以尽可能低的延迟完成任务。它的本质就是以序列化的方式处理任务。

GPU 对吞吐量方面进行了优化，它允许一次推尽可能多的任务到内部，并通过并行方式处理每个任务。

下面的示例图显示了 CPU 和 GPU **核心** 的数量。显而易见地，GPU 具有更多的核心来处理一个任务。

![cpu-vs-gpu-cores](/images/GPU-101-Architecture/01-cpu-vs-gpu-cores.png)

## 异同点

然而，这不仅仅是核心数量的问题。而当我们说到NVIDIA GPU中的核心时，我们指的是由ALU（算术逻辑单元）组成的CUDA核心。不同厂商的术语可能会有所不同。

从CPU和GPU的整体架构来看，我们可以看到两者之间有很多相似之处。两者都使用了缓存层、内存控制器和全局内存的内存构造。对现代CPU架构的高层概述表明，它都是通过使用重要的缓存内存层来实现低延迟的内存访问。让我们先看一张图，它显示了一个通用的、以内存为中心的现代CPU封装（注意：精确的布局强烈地取决于供应商/型号）。

![cpu-hl-architecture](/images/GPU-101-Architecture/02-cpu-hl-architecture.png)

一个CPU包由核心组成，包含独立的数据层和指令层-1缓存，由层-2缓存支持。第3层高速缓存，也就是最后一层高速缓存，由多个核心共享。如果数据没有驻留在缓存层中，它将从全局DDR-4内存中获取数据。每个CPU的核心数量可以达到28个或32个，根据品牌和型号的不同，在Turbo模式下可以运行到2.5GHz或3.8GHz。缓存大小范围为每个核心最高2MB二级缓存。

## 探索GPU架构

如果我们检查GPU的高层架构概述（同样，强烈依赖make/model），看起来GPU的本质就是把可用的核心投入工作，它不太注重低延迟的缓存内存访问。

![gpu-architecture](/images/GPU-101-Architecture/03-gpu-architecture.png)

单个GPU设备由多个处理器集群（PC）组成，其中包含多个流式多处理器（SM）。每个SM容纳一个第1层指令缓存层与其相关的核心。通常情况下，一个SM在从全局GDDR-5内存中提取数据之前，会使用一个专用的第1层缓存和一个共享的第2层缓存。其架构对内存延迟的容忍度很高。

与CPU相比，GPU工作的内存缓存层数较少，而且相对较小。原因是GPU有更多的晶体管用于计算，这意味着它不太在乎从内存中检索数据所需的时间。只要GPU手头有足够的计算量，潜在的内存访问 "延迟 "就会被掩盖，使其保持忙碌。

> A GPU is optimized for data parallel throughput computations.

从核心数量来看，它很快就能让你看到它在并行方面的可能性。 当检查当前NVIDIA的旗舰产品Tesla V100时，一台设备包含80个SM，每个SM包含64个核心，总共有5120个核心！任务不是安排给单个核心，而是安排给处理器集群和SM。任务不是安排给单个核心，而是安排给处理器集群和SM。这就是它能够并行处理的原因。现在把这个强大的硬件设备和编程框架结合起来，应用就可以充分发挥GPU的计算能力。

## 参考资料

* [Exploring the GPU Architecture](https://nielshagoort.com/2019/03/12/exploring-the-gpu-architecture/)
